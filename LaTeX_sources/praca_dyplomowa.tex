\documentclass{article}
 
\begin{document}
\title{
	An application for tracking the flow of resources for Bitcoin cryptocurrency \\
}
\date{}
\author{Marcin Pieczka}
\maketitle

\section{Goals}
Every process of analyzing data starts with accessing the data, when analyzing Bitcoin blockchain, this first step might be the hardest one. Currently Bitcoin blockchain contains over 160GB of raw, binary data and everyone who attempts to analyze it will have to have an efficient and reliable way to work with it.
\\
\\
My goal is to create application that: 
\begin{itemize}

\item
allows fast access to blockchain data
\item
allows access over the network
\item
updates its data in constant manner
\item
provides API for Python and R
\item
is easy to install

\end{itemize}

\section{Alternative software providing similar features}
\subsubsection*{blockchain.info}

Web application blockchain.info provides free access to Bitcoin blockchain data by either website, JSON API or API's dedicated to specific languages including Python, but without support for R. Number of requests is limited.
\\
\\
Relevant API's provided by blockchain.info:
\begin{itemize}
\item
getting single block, by block hash
\item
getting single block, by hight
\item
getting multiple block headers
\item
getting single transaction, by transaction hash
\item
getting all transactions of single or multiple addresses


\end{itemize}

Most common usage scenario in analytic context is getting range of blocks, for example all block from 10.04.2017 to 20.04.2017 nonetheless blockchain.info does not provide simple way to get such data. To achieve this we would have to make thousands of requests to API providing us with single block data, and this wouldn't be fast enough.

Other problem is the API call limit that can make usage not only slow, but impossible. 

\section{Deployment and packaging}
\subsubsection*{Docker - for and against}

The main things that can be accomplished with well chosen deployment and packaging are simplicity of use and cross-platform possibilities witch can positively influence adoption rate of this solution in perspective of both usage and further open source development.

Docker is known for achieving those goals, but not everything about Docker will be helpful. My main concern is running Bitcoin Core in Docker Image, it would be a great inconvenience to people who already have full node on their machine and this problem has to be dealt with.

My proposal is to have Bitcoin Core installed separately, and create simple HTTP server that would provide access to block data, the rest of the application would by in Docker. Creation of such server can be achieved with running command  
\begin{verbatim}
python -m http.server 8000 --bind 127.0.0.1
\end{verbatim}
With that we can keep the advantages of using docker, and provide a new possibility to the user - keeping Bitcoin Core on separate machine witch when considering that both parts of the application, Bitcoin Core and database with API will be well over 100Gb in size might be a big advantage.

\section{Database}
\subsection*{Database operations}
My least concern are operations that modify the data, and their performance will not be taken into consideration

Main goal is to enable fast querying of the blocks by block hash, time and other block attributes, and to be able to return specified amount of consecutive blocks starting or ending with certain block. 

Additionally it might be necessary to enable fast querying for transactions or blocks containing transactions of certain addresses but further research of what users need in this regard is needed

\subsection{Database choice}
At the beginning lets simplify the choice between RDBMS and NoSQL databases. Out of many NoSQL possibilities I have chosen MongoDB database based on some quick research of different NoSQL systems strengths and weaknesses.

Lets lay out some facts that will help to decide whether to use relational database, or MongoDB

\begin{itemize}
\item 
To achieve fast querying, the data will be strongly denormalized
\item 
You can achieve comparable performance from MongoDB and some RDBMS but Mongo seams to make storing denormalized data idiomatic and RDBMS with highly denormalized data just don't feel right
\item MongoDB fully supports JSON, witch will be the format of data received by end user
\end{itemize}

Based on these facts I will use as my database MongoDB. This problem seams like a perfect usage for database of such type because of its denormalized nature and native support for JSON
 \subsection{Database structure}
 
 At this point I propose having one collection of blocks, each document containing all block attributes like hashes, time, transactions list and others
 
Indexing increases performance of querying the data and hinders the performance of operations like adding and removing data witch in this case looks like a great bargain. There might be additional memory cost associated with indexes, but this should not be a problem.

The indexes will be added to fields like block hash, time or height, adding indexing to transaction list is also a possibility and will be considered and tested. With indexes on transaction list it should be possible to quickly query for blocks containing transactions in witch given address receives or sends bitcoin, but its hard for me to speculate about this matter without thorough testing in live system.
\end{document}

