\documentclass{article}
 
\begin{document}
\title{
	An application for tracking the flow of resources for Bitcoin cryptocurrency \\
}
\date{}
\author{Marcin Pieczka}
\maketitle

\section{Goals}
Every process of analyzing data starts with accessing the data, when analyzing Bitcoin blockchain, this first step might be the hardest one. Currently Bitcoin blockchain contains over 160GB of raw, binary data and everyone who attempts to analyze it will have to have an efficient and reliable way to work with it.
\\
\\
My goal is to create application that: 
\begin{itemize}

\item
allows fast access to blockchain data
\item
allows access over the network
\item
updates its data in constant manner
\item
provides API for Python and R
\item
is easy to install

\end{itemize}

\section{Alternative software providing similar features}
\subsubsection*{blockchain.info}

Web application blockchain.info provides free access to Bitcoin blockchain data by either website, JSON API or API's dedicated to specific languages including Python, but without support for R. Number of requests is limited.
\\
\\
Relevant API's provided by blockchain.info:
\begin{itemize}
\item
getting single block, by block hash
\item
getting single block, by hight
\item
getting multiple block headers
\item
getting single transaction, by transaction hash
\item
getting all transactions of single or multiple addresses


\end{itemize}

Most common usage scenario in analytic context is getting range of blocks, for example all block from 10.04.2017 to 20.04.2017 nonetheless blockchain.info does not provide simple way to get such data. To achieve this we would have to make thousands of requests to API providing us with single block data, and this wouldn't be fast enough.

Other and the biggest problem is the API call limit that would make working with this application impossible for larger query's

\subsubsection*{blockexplorer.com}

This web application is very similar to blockchain.info in almost every aspect, although there are some differences. Data is accessible either by website or by JSON API, but there is no Python or R API provided. Set of API's is almost identical to blockchain.info, and does not provide easy way to get multiple blocks. The main difference is that there is no official API call limit, but using tool that is not controlled by us means that such limit can appear every moment.

\subsubsection*{libbitcoin-database}

\subsubsection*{BitcoinDatabaseGenerator}


\section{Design}

\subsection{System inputs and outputs}

In this section I will specify inputs and outputs of the system, witch will then help me to discover what transformation the input data will undergo, and what components are needed to provide outputs efficiently

\subsubsection{Inputs}
\paragraph{Bitcoind BLK files}
The only source of bitcoin block data will be BLK files stored by full bitcoin node. Daemon process bitcoind gets blocks from neighboring nodes and stores them in data directory. The blk files in default configuration store up to 128MB of raw network format block data, and blocks are stored in order in witch they came from network. There exist multiple library's that handle parsing these files, so accessing this data will not be a problem.
\\
\\
One important thing to mention about bitcoin transactions stored in blocks is the fact that address from witch the transaction comes from is described as output of some previous transaction.

\paragraph{Request for blocks} will be the way user communicates with the system. In this request user will specify witch blocks he wants to receive, usually it will be blocks from specified range of time, or range of height.

\subsubsection{Outputs}
\paragraph{Response on request for blocks} will contain block data in format understandable by user 

\subsubsection*{Docker - for and against}

The main things that can be accomplished with well chosen deployment and packaging are simplicity of use and cross-platform possibilities witch can positively influence adoption rate of this solution in perspective of both usage and further open source development.

Docker is known for achieving those goals, but not everything about Docker will be helpful. My main concern is running Bitcoin Core in Docker Image, it would be a great inconvenience to people who already have full node on their machine and this problem has to be dealt with.

My proposal is to have Bitcoin Core installed separately, and create simple HTTP server that would provide access to block data, the rest of the application would by in Docker. Creation of such server can be achieved with running command  
\begin{verbatim}
python -m http.server 8000 --bind 127.0.0.1
\end{verbatim}
With that we can keep the advantages of using docker, and provide a new possibility to the user - keeping Bitcoin Core on separate machine witch when considering that both parts of the application, Bitcoin Core and database with API will be well over 100Gb in size might be a big advantage.

\section{Database}
\subsection*{Database operations}
My least concern are operations that modify the data, and their performance will not be taken into consideration

Main goal is to enable fast querying of the blocks by block hash, time and other block attributes, and to be able to return specified amount of consecutive blocks starting or ending with certain block. 

Additionally it might be necessary to enable fast querying for transactions or blocks containing transactions of certain addresses but further research of what users need in this regard is needed

\subsection{Database choice}
At the beginning lets simplify the choice between RDBMS and NoSQL databases. Out of many NoSQL possibilities I have chosen MongoDB database based on some quick research of different NoSQL systems strengths and weaknesses.

Lets lay out some facts that will help to decide whether to use relational database, or MongoDB

\begin{itemize}
\item 
To achieve fast querying, the data will be strongly denormalized
\item 
You can achieve comparable performance from MongoDB and some RDBMS but Mongo seams to make storing denormalized data idiomatic and RDBMS with highly denormalized data just don't feel right
\item MongoDB fully supports JSON, witch will be the format of data received by end user
\end{itemize}

Based on these facts I will use as my database MongoDB. This problem seams like a perfect usage for database of such type because of its denormalized nature and native support for JSON
 \subsection{Database structure}
 
 At this point I propose having one collection of blocks, each document containing all block attributes like hashes, time, transactions list and others
 
Indexing increases performance of querying the data and hinders the performance of operations like adding and removing data witch in this case looks like a great bargain. There might be additional memory cost associated with indexes, but this should not be a problem.

The indexes will be added to fields like block hash, time or height, adding indexing to transaction list is also a possibility and will be considered and tested. With indexes on transaction list it should be possible to quickly query for blocks containing transactions in witch given address receives or sends bitcoin, but its hard for me to speculate about this matter without thorough testing in live system.
\end{document}

